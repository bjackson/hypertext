model:
  name: "google/byt5-small"
  max_input_length: 128
  max_output_length: 128

training:
  batch_size: 16
  gradient_accumulation_steps: 4
  learning_rate: 5e-5
  warmup_steps: 1000
  num_epochs: 3
  weight_decay: 0.01
  max_grad_norm: 1.0

data:
  train_path: "data/train.jsonl"
  val_path: "data/val.jsonl"
  num_workers: 4

checkpointing:
  save_strategy: "epoch"
  save_total_limit: 3
  output_dir: "models/byt5-shorthand-v1"

logging:
  logging_steps: 100
  eval_steps: 500
  save_steps: 500

seed: 42
